{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PyTorch CIFAR Improved.ipynb","provenance":[],"authorship_tag":"ABX9TyMDriDuw0AbvU9n1A8zl7Qu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"cfd9bd95a1214410aacb89ee00cbe327":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_65a1e7ed63234929ae8fb5d10ada8a62","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9519bf09213b4e42b0fcba7ef48cf6ba","IPY_MODEL_8b9d4fe44bf24292940dd98e9f467716"]}},"65a1e7ed63234929ae8fb5d10ada8a62":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9519bf09213b4e42b0fcba7ef48cf6ba":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0283b2471dcb49e0b6472f192fdb0dcc","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":170498071,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":170498071,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_df5cb3eb680c4d219415dbe3cf60aca9"}},"8b9d4fe44bf24292940dd98e9f467716":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_396f3707b1e74635b654a69f9dbd33f0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 170499072/? [00:07&lt;00:00, 22169338.75it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4c8d5a2f1cd64dfd95536c4a749f0caf"}},"0283b2471dcb49e0b6472f192fdb0dcc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"df5cb3eb680c4d219415dbe3cf60aca9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"396f3707b1e74635b654a69f9dbd33f0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4c8d5a2f1cd64dfd95536c4a749f0caf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"DpK8OZjvz8TZ"},"source":["Similar to the \"PyTorch CIFAR\" notebook with two improvements:\n","\n","\n","1.   This notebook uses a much larger CNN\n","2.   This notebook uses data augmentation, where I simulate additional photos at new angles/perspectives using transforms"]},{"cell_type":"code","metadata":{"id":"T_3nZwkDz51f","executionInfo":{"status":"ok","timestamp":1626164769624,"user_tz":-60,"elapsed":4580,"user":{"displayName":"Jacob Oliver","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_d3WZpZuOYfhw8lFJT1qT-x68z0UdHQ2wGVSH=s64","userId":"00067215047497340995"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from datetime import datetime"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":117,"referenced_widgets":["cfd9bd95a1214410aacb89ee00cbe327","65a1e7ed63234929ae8fb5d10ada8a62","9519bf09213b4e42b0fcba7ef48cf6ba","8b9d4fe44bf24292940dd98e9f467716","0283b2471dcb49e0b6472f192fdb0dcc","df5cb3eb680c4d219415dbe3cf60aca9","396f3707b1e74635b654a69f9dbd33f0","4c8d5a2f1cd64dfd95536c4a749f0caf"]},"id":"pq066oUY0Qem","executionInfo":{"status":"ok","timestamp":1626164784434,"user_tz":-60,"elapsed":14818,"user":{"displayName":"Jacob Oliver","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_d3WZpZuOYfhw8lFJT1qT-x68z0UdHQ2wGVSH=s64","userId":"00067215047497340995"}},"outputId":"02c68f50-ed10-4319-ef8e-6db9651ab8ca"},"source":["# examples: https://pytorch.org/docs/stable/torchvision/transforms.html\n","transformer_train = torchvision.transforms.Compose([\n","    # torchvision.transforms.ColorJitter(\n","        #brightness=0.2, contrast=0.2, stauration=0.2, hue=0.2),\n","    transforms.RandomCrop(32, padding=4),\n","    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n","    # torchvision.transforms.RandomRotation(degrees=15),\n","    torchvision.transforms.RandomAffine(0, translate=(0.1, 0.1)),\n","    # torchvision.transforms.RandomPerspective(),\n","    transforms.ToTensor(),\n","])\n","\n","train_dataset = torchvision.datasets.CIFAR10(\n","    root='.',\n","    train=True,\n","    transform=transformer_train,\n","    download=True\n",")\n","test_dataset = torchvision.datasets.CIFAR10(\n","    root='.',\n","    train=False,\n","    transform=transforms.ToTensor(),\n","    download=True\n",")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cfd9bd95a1214410aacb89ee00cbe327","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Extracting ./cifar-10-python.tar.gz to .\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ysYP6Ts_1GPR","executionInfo":{"status":"ok","timestamp":1626164784437,"user_tz":-60,"elapsed":47,"user":{"displayName":"Jacob Oliver","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_d3WZpZuOYfhw8lFJT1qT-x68z0UdHQ2wGVSH=s64","userId":"00067215047497340995"}},"outputId":"af9a2998-0d1a-4b70-eccf-e2d465cbe1ca"},"source":["# This dataset behaves differently from MNIST / Fashion MNIST\n","# it is a Numpy array!\n","train_dataset.data"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[[ 59,  62,  63],\n","         [ 43,  46,  45],\n","         [ 50,  48,  43],\n","         ...,\n","         [158, 132, 108],\n","         [152, 125, 102],\n","         [148, 124, 103]],\n","\n","        [[ 16,  20,  20],\n","         [  0,   0,   0],\n","         [ 18,   8,   0],\n","         ...,\n","         [123,  88,  55],\n","         [119,  83,  50],\n","         [122,  87,  57]],\n","\n","        [[ 25,  24,  21],\n","         [ 16,   7,   0],\n","         [ 49,  27,   8],\n","         ...,\n","         [118,  84,  50],\n","         [120,  84,  50],\n","         [109,  73,  42]],\n","\n","        ...,\n","\n","        [[208, 170,  96],\n","         [201, 153,  34],\n","         [198, 161,  26],\n","         ...,\n","         [160, 133,  70],\n","         [ 56,  31,   7],\n","         [ 53,  34,  20]],\n","\n","        [[180, 139,  96],\n","         [173, 123,  42],\n","         [186, 144,  30],\n","         ...,\n","         [184, 148,  94],\n","         [ 97,  62,  34],\n","         [ 83,  53,  34]],\n","\n","        [[177, 144, 116],\n","         [168, 129,  94],\n","         [179, 142,  87],\n","         ...,\n","         [216, 184, 140],\n","         [151, 118,  84],\n","         [123,  92,  72]]],\n","\n","\n","       [[[154, 177, 187],\n","         [126, 137, 136],\n","         [105, 104,  95],\n","         ...,\n","         [ 91,  95,  71],\n","         [ 87,  90,  71],\n","         [ 79,  81,  70]],\n","\n","        [[140, 160, 169],\n","         [145, 153, 154],\n","         [125, 125, 118],\n","         ...,\n","         [ 96,  99,  78],\n","         [ 77,  80,  62],\n","         [ 71,  73,  61]],\n","\n","        [[140, 155, 164],\n","         [139, 146, 149],\n","         [115, 115, 112],\n","         ...,\n","         [ 79,  82,  64],\n","         [ 68,  70,  55],\n","         [ 67,  69,  55]],\n","\n","        ...,\n","\n","        [[175, 167, 166],\n","         [156, 154, 160],\n","         [154, 160, 170],\n","         ...,\n","         [ 42,  34,  36],\n","         [ 61,  53,  57],\n","         [ 93,  83,  91]],\n","\n","        [[165, 154, 128],\n","         [156, 152, 130],\n","         [159, 161, 142],\n","         ...,\n","         [103,  93,  96],\n","         [123, 114, 120],\n","         [131, 121, 131]],\n","\n","        [[163, 148, 120],\n","         [158, 148, 122],\n","         [163, 156, 133],\n","         ...,\n","         [143, 133, 139],\n","         [143, 134, 142],\n","         [143, 133, 144]]],\n","\n","\n","       [[[255, 255, 255],\n","         [253, 253, 253],\n","         [253, 253, 253],\n","         ...,\n","         [253, 253, 253],\n","         [253, 253, 253],\n","         [253, 253, 253]],\n","\n","        [[255, 255, 255],\n","         [255, 255, 255],\n","         [255, 255, 255],\n","         ...,\n","         [255, 255, 255],\n","         [255, 255, 255],\n","         [255, 255, 255]],\n","\n","        [[255, 255, 255],\n","         [254, 254, 254],\n","         [254, 254, 254],\n","         ...,\n","         [254, 254, 254],\n","         [254, 254, 254],\n","         [254, 254, 254]],\n","\n","        ...,\n","\n","        [[113, 120, 112],\n","         [111, 118, 111],\n","         [105, 112, 106],\n","         ...,\n","         [ 72,  81,  80],\n","         [ 72,  80,  79],\n","         [ 72,  80,  79]],\n","\n","        [[111, 118, 110],\n","         [104, 111, 104],\n","         [ 99, 106,  98],\n","         ...,\n","         [ 68,  75,  73],\n","         [ 70,  76,  75],\n","         [ 78,  84,  82]],\n","\n","        [[106, 113, 105],\n","         [ 99, 106,  98],\n","         [ 95, 102,  94],\n","         ...,\n","         [ 78,  85,  83],\n","         [ 79,  85,  83],\n","         [ 80,  86,  84]]],\n","\n","\n","       ...,\n","\n","\n","       [[[ 35, 178, 235],\n","         [ 40, 176, 239],\n","         [ 42, 176, 241],\n","         ...,\n","         [ 99, 177, 219],\n","         [ 79, 147, 197],\n","         [ 89, 148, 189]],\n","\n","        [[ 57, 182, 234],\n","         [ 44, 184, 250],\n","         [ 50, 183, 240],\n","         ...,\n","         [156, 182, 200],\n","         [141, 177, 206],\n","         [116, 149, 175]],\n","\n","        [[ 98, 197, 237],\n","         [ 64, 189, 252],\n","         [ 69, 192, 245],\n","         ...,\n","         [188, 195, 206],\n","         [119, 135, 147],\n","         [ 61,  79,  90]],\n","\n","        ...,\n","\n","        [[ 73,  79,  77],\n","         [ 53,  63,  68],\n","         [ 54,  68,  80],\n","         ...,\n","         [ 17,  40,  64],\n","         [ 21,  36,  51],\n","         [ 33,  48,  49]],\n","\n","        [[ 61,  68,  75],\n","         [ 55,  70,  86],\n","         [ 57,  79, 103],\n","         ...,\n","         [ 24,  48,  72],\n","         [ 17,  35,  53],\n","         [  7,  23,  32]],\n","\n","        [[ 44,  56,  73],\n","         [ 46,  66,  88],\n","         [ 49,  77, 105],\n","         ...,\n","         [ 27,  52,  77],\n","         [ 21,  43,  66],\n","         [ 12,  31,  50]]],\n","\n","\n","       [[[189, 211, 240],\n","         [186, 208, 236],\n","         [185, 207, 235],\n","         ...,\n","         [175, 195, 224],\n","         [172, 194, 222],\n","         [169, 194, 220]],\n","\n","        [[194, 210, 239],\n","         [191, 207, 236],\n","         [190, 206, 235],\n","         ...,\n","         [173, 192, 220],\n","         [171, 191, 218],\n","         [167, 190, 216]],\n","\n","        [[208, 219, 244],\n","         [205, 216, 240],\n","         [204, 215, 239],\n","         ...,\n","         [175, 191, 217],\n","         [172, 190, 216],\n","         [169, 191, 215]],\n","\n","        ...,\n","\n","        [[207, 199, 181],\n","         [203, 195, 175],\n","         [203, 196, 173],\n","         ...,\n","         [135, 132, 127],\n","         [162, 158, 150],\n","         [168, 163, 151]],\n","\n","        [[198, 190, 170],\n","         [189, 181, 159],\n","         [180, 172, 147],\n","         ...,\n","         [178, 171, 160],\n","         [175, 169, 156],\n","         [175, 169, 154]],\n","\n","        [[198, 189, 173],\n","         [189, 181, 162],\n","         [178, 170, 149],\n","         ...,\n","         [195, 184, 169],\n","         [196, 189, 171],\n","         [195, 190, 171]]],\n","\n","\n","       [[[229, 229, 239],\n","         [236, 237, 247],\n","         [234, 236, 247],\n","         ...,\n","         [217, 219, 233],\n","         [221, 223, 234],\n","         [222, 223, 233]],\n","\n","        [[222, 221, 229],\n","         [239, 239, 249],\n","         [233, 234, 246],\n","         ...,\n","         [223, 223, 236],\n","         [227, 228, 238],\n","         [210, 211, 220]],\n","\n","        [[213, 206, 211],\n","         [234, 232, 239],\n","         [231, 233, 244],\n","         ...,\n","         [220, 220, 232],\n","         [220, 219, 232],\n","         [202, 203, 215]],\n","\n","        ...,\n","\n","        [[150, 143, 135],\n","         [140, 135, 127],\n","         [132, 127, 120],\n","         ...,\n","         [224, 222, 218],\n","         [230, 228, 225],\n","         [241, 241, 238]],\n","\n","        [[137, 132, 126],\n","         [130, 127, 120],\n","         [125, 121, 115],\n","         ...,\n","         [181, 180, 178],\n","         [202, 201, 198],\n","         [212, 211, 207]],\n","\n","        [[122, 119, 114],\n","         [118, 116, 110],\n","         [120, 116, 111],\n","         ...,\n","         [179, 177, 173],\n","         [164, 164, 162],\n","         [163, 163, 161]]]], dtype=uint8)"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-szLz1Ex1XNP","executionInfo":{"status":"ok","timestamp":1626164784437,"user_tz":-60,"elapsed":40,"user":{"displayName":"Jacob Oliver","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_d3WZpZuOYfhw8lFJT1qT-x68z0UdHQ2wGVSH=s64","userId":"00067215047497340995"}},"outputId":"255edd87-ad0e-43a2-fdfc-248c45212c53"},"source":["# Now working with colour images\n","train_dataset.data.shape"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(50000, 32, 32, 3)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SLGims_61aRl","executionInfo":{"status":"ok","timestamp":1626164784438,"user_tz":-60,"elapsed":25,"user":{"displayName":"Jacob Oliver","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_d3WZpZuOYfhw8lFJT1qT-x68z0UdHQ2wGVSH=s64","userId":"00067215047497340995"}},"outputId":"43f9173d-acb4-4104-ad5c-f1f33f2310d5"},"source":["# Behaves differently from MNIST / Fashion MNIST\n","# It is a list!\n","train_dataset.targets"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[6,\n"," 9,\n"," 9,\n"," 4,\n"," 1,\n"," 1,\n"," 2,\n"," 7,\n"," 8,\n"," 3,\n"," 4,\n"," 7,\n"," 7,\n"," 2,\n"," 9,\n"," 9,\n"," 9,\n"," 3,\n"," 2,\n"," 6,\n"," 4,\n"," 3,\n"," 6,\n"," 6,\n"," 2,\n"," 6,\n"," 3,\n"," 5,\n"," 4,\n"," 0,\n"," 0,\n"," 9,\n"," 1,\n"," 3,\n"," 4,\n"," 0,\n"," 3,\n"," 7,\n"," 3,\n"," 3,\n"," 5,\n"," 2,\n"," 2,\n"," 7,\n"," 1,\n"," 1,\n"," 1,\n"," 2,\n"," 2,\n"," 0,\n"," 9,\n"," 5,\n"," 7,\n"," 9,\n"," 2,\n"," 2,\n"," 5,\n"," 2,\n"," 4,\n"," 3,\n"," 1,\n"," 1,\n"," 8,\n"," 2,\n"," 1,\n"," 1,\n"," 4,\n"," 9,\n"," 7,\n"," 8,\n"," 5,\n"," 9,\n"," 6,\n"," 7,\n"," 3,\n"," 1,\n"," 9,\n"," 0,\n"," 3,\n"," 1,\n"," 3,\n"," 5,\n"," 4,\n"," 5,\n"," 7,\n"," 7,\n"," 4,\n"," 7,\n"," 9,\n"," 4,\n"," 2,\n"," 3,\n"," 8,\n"," 0,\n"," 1,\n"," 6,\n"," 1,\n"," 1,\n"," 4,\n"," 1,\n"," 8,\n"," 3,\n"," 9,\n"," 6,\n"," 6,\n"," 1,\n"," 8,\n"," 5,\n"," 2,\n"," 9,\n"," 9,\n"," 8,\n"," 1,\n"," 7,\n"," 7,\n"," 0,\n"," 0,\n"," 6,\n"," 9,\n"," 1,\n"," 2,\n"," 2,\n"," 9,\n"," 2,\n"," 6,\n"," 6,\n"," 1,\n"," 9,\n"," 5,\n"," 0,\n"," 4,\n"," 7,\n"," 6,\n"," 7,\n"," 1,\n"," 8,\n"," 1,\n"," 1,\n"," 2,\n"," 8,\n"," 1,\n"," 3,\n"," 3,\n"," 6,\n"," 2,\n"," 4,\n"," 9,\n"," 9,\n"," 5,\n"," 4,\n"," 3,\n"," 6,\n"," 7,\n"," 4,\n"," 6,\n"," 8,\n"," 5,\n"," 5,\n"," 4,\n"," 3,\n"," 1,\n"," 8,\n"," 4,\n"," 7,\n"," 6,\n"," 0,\n"," 9,\n"," 5,\n"," 1,\n"," 3,\n"," 8,\n"," 2,\n"," 7,\n"," 5,\n"," 3,\n"," 4,\n"," 1,\n"," 5,\n"," 7,\n"," 0,\n"," 4,\n"," 7,\n"," 5,\n"," 5,\n"," 1,\n"," 0,\n"," 9,\n"," 6,\n"," 9,\n"," 0,\n"," 8,\n"," 7,\n"," 8,\n"," 8,\n"," 2,\n"," 5,\n"," 2,\n"," 3,\n"," 5,\n"," 0,\n"," 6,\n"," 1,\n"," 9,\n"," 3,\n"," 6,\n"," 9,\n"," 1,\n"," 3,\n"," 9,\n"," 6,\n"," 6,\n"," 7,\n"," 1,\n"," 0,\n"," 9,\n"," 5,\n"," 8,\n"," 5,\n"," 2,\n"," 9,\n"," 0,\n"," 8,\n"," 8,\n"," 0,\n"," 6,\n"," 9,\n"," 1,\n"," 1,\n"," 6,\n"," 3,\n"," 7,\n"," 6,\n"," 6,\n"," 0,\n"," 6,\n"," 6,\n"," 1,\n"," 7,\n"," 1,\n"," 5,\n"," 8,\n"," 3,\n"," 6,\n"," 6,\n"," 8,\n"," 6,\n"," 8,\n"," 4,\n"," 6,\n"," 6,\n"," 1,\n"," 3,\n"," 8,\n"," 3,\n"," 4,\n"," 1,\n"," 7,\n"," 1,\n"," 3,\n"," 8,\n"," 5,\n"," 1,\n"," 1,\n"," 4,\n"," 0,\n"," 9,\n"," 3,\n"," 7,\n"," 4,\n"," 9,\n"," 9,\n"," 2,\n"," 4,\n"," 9,\n"," 9,\n"," 1,\n"," 0,\n"," 5,\n"," 9,\n"," 0,\n"," 8,\n"," 2,\n"," 1,\n"," 2,\n"," 0,\n"," 5,\n"," 6,\n"," 3,\n"," 2,\n"," 7,\n"," 8,\n"," 8,\n"," 6,\n"," 0,\n"," 7,\n"," 9,\n"," 4,\n"," 5,\n"," 6,\n"," 4,\n"," 2,\n"," 1,\n"," 1,\n"," 2,\n"," 1,\n"," 5,\n"," 9,\n"," 9,\n"," 0,\n"," 8,\n"," 4,\n"," 1,\n"," 1,\n"," 6,\n"," 3,\n"," 3,\n"," 9,\n"," 0,\n"," 7,\n"," 9,\n"," 7,\n"," 7,\n"," 9,\n"," 1,\n"," 5,\n"," 1,\n"," 6,\n"," 6,\n"," 8,\n"," 7,\n"," 1,\n"," 3,\n"," 0,\n"," 3,\n"," 3,\n"," 2,\n"," 4,\n"," 5,\n"," 7,\n"," 5,\n"," 9,\n"," 0,\n"," 3,\n"," 4,\n"," 0,\n"," 4,\n"," 4,\n"," 6,\n"," 0,\n"," 0,\n"," 6,\n"," 6,\n"," 0,\n"," 8,\n"," 1,\n"," 6,\n"," 2,\n"," 9,\n"," 2,\n"," 5,\n"," 9,\n"," 6,\n"," 7,\n"," 4,\n"," 1,\n"," 8,\n"," 7,\n"," 3,\n"," 6,\n"," 9,\n"," 3,\n"," 0,\n"," 4,\n"," 0,\n"," 5,\n"," 1,\n"," 0,\n"," 3,\n"," 4,\n"," 8,\n"," 5,\n"," 4,\n"," 7,\n"," 2,\n"," 3,\n"," 9,\n"," 7,\n"," 6,\n"," 7,\n"," 1,\n"," 4,\n"," 7,\n"," 0,\n"," 1,\n"," 7,\n"," 3,\n"," 1,\n"," 8,\n"," 4,\n"," 4,\n"," 2,\n"," 0,\n"," 2,\n"," 2,\n"," 0,\n"," 0,\n"," 9,\n"," 0,\n"," 9,\n"," 6,\n"," 8,\n"," 2,\n"," 7,\n"," 7,\n"," 4,\n"," 0,\n"," 3,\n"," 0,\n"," 8,\n"," 9,\n"," 4,\n"," 2,\n"," 7,\n"," 2,\n"," 5,\n"," 2,\n"," 5,\n"," 1,\n"," 9,\n"," 4,\n"," 8,\n"," 5,\n"," 1,\n"," 7,\n"," 4,\n"," 4,\n"," 0,\n"," 6,\n"," 9,\n"," 0,\n"," 7,\n"," 8,\n"," 8,\n"," 9,\n"," 9,\n"," 3,\n"," 3,\n"," 4,\n"," 0,\n"," 4,\n"," 5,\n"," 6,\n"," 6,\n"," 0,\n"," 1,\n"," 0,\n"," 8,\n"," 0,\n"," 4,\n"," 8,\n"," 8,\n"," 1,\n"," 5,\n"," 2,\n"," 6,\n"," 8,\n"," 1,\n"," 0,\n"," 0,\n"," 7,\n"," 7,\n"," 5,\n"," 9,\n"," 6,\n"," 2,\n"," 8,\n"," 3,\n"," 4,\n"," 7,\n"," 3,\n"," 9,\n"," 0,\n"," 1,\n"," 2,\n"," 4,\n"," 8,\n"," 1,\n"," 8,\n"," 6,\n"," 4,\n"," 4,\n"," 5,\n"," 7,\n"," 1,\n"," 3,\n"," 9,\n"," 8,\n"," 0,\n"," 1,\n"," 7,\n"," 5,\n"," 8,\n"," 2,\n"," 8,\n"," 0,\n"," 4,\n"," 1,\n"," 8,\n"," 9,\n"," 8,\n"," 2,\n"," 9,\n"," 9,\n"," 2,\n"," 7,\n"," 5,\n"," 7,\n"," 3,\n"," 8,\n"," 8,\n"," 4,\n"," 4,\n"," 2,\n"," 7,\n"," 1,\n"," 6,\n"," 4,\n"," 0,\n"," 4,\n"," 6,\n"," 9,\n"," 7,\n"," 6,\n"," 2,\n"," 5,\n"," 5,\n"," 1,\n"," 7,\n"," 2,\n"," 2,\n"," 2,\n"," 9,\n"," 5,\n"," 4,\n"," 2,\n"," 7,\n"," 8,\n"," 1,\n"," 3,\n"," 4,\n"," 3,\n"," 7,\n"," 6,\n"," 9,\n"," 8,\n"," 0,\n"," 6,\n"," 0,\n"," 2,\n"," 2,\n"," 2,\n"," 1,\n"," 8,\n"," 4,\n"," 0,\n"," 1,\n"," 8,\n"," 8,\n"," 1,\n"," 5,\n"," 7,\n"," 6,\n"," 4,\n"," 5,\n"," 8,\n"," 7,\n"," 1,\n"," 9,\n"," 1,\n"," 9,\n"," 8,\n"," 4,\n"," 7,\n"," 3,\n"," 8,\n"," 8,\n"," 2,\n"," 6,\n"," 6,\n"," 7,\n"," 1,\n"," 6,\n"," 8,\n"," 1,\n"," 9,\n"," 7,\n"," 8,\n"," 3,\n"," 0,\n"," 1,\n"," 0,\n"," 8,\n"," 8,\n"," 3,\n"," 0,\n"," 0,\n"," 1,\n"," 5,\n"," 0,\n"," 8,\n"," 8,\n"," 7,\n"," 9,\n"," 9,\n"," 0,\n"," 9,\n"," 4,\n"," 1,\n"," 3,\n"," 6,\n"," 6,\n"," 4,\n"," 4,\n"," 7,\n"," 5,\n"," 6,\n"," 0,\n"," 8,\n"," 0,\n"," 3,\n"," 2,\n"," 8,\n"," 4,\n"," 6,\n"," 9,\n"," 9,\n"," 7,\n"," 0,\n"," 3,\n"," 3,\n"," 6,\n"," 7,\n"," 4,\n"," 9,\n"," 1,\n"," 6,\n"," 2,\n"," 7,\n"," 2,\n"," 2,\n"," 0,\n"," 6,\n"," 7,\n"," 5,\n"," 7,\n"," 6,\n"," 8,\n"," 9,\n"," 0,\n"," 9,\n"," 4,\n"," 4,\n"," 7,\n"," 0,\n"," 9,\n"," 4,\n"," 9,\n"," 6,\n"," 9,\n"," 4,\n"," 5,\n"," 7,\n"," 9,\n"," 2,\n"," 4,\n"," 5,\n"," 1,\n"," 4,\n"," 3,\n"," 9,\n"," 6,\n"," 5,\n"," 6,\n"," 9,\n"," 3,\n"," 3,\n"," 5,\n"," 0,\n"," 7,\n"," 2,\n"," 1,\n"," 3,\n"," 6,\n"," 4,\n"," 0,\n"," 0,\n"," 2,\n"," 5,\n"," 0,\n"," 1,\n"," 0,\n"," 2,\n"," 3,\n"," 9,\n"," 8,\n"," 4,\n"," 9,\n"," 8,\n"," 0,\n"," 2,\n"," 6,\n"," 4,\n"," 4,\n"," 0,\n"," 1,\n"," 8,\n"," 8,\n"," 3,\n"," 6,\n"," 9,\n"," 6,\n"," 6,\n"," 7,\n"," 8,\n"," 2,\n"," 4,\n"," 5,\n"," 7,\n"," 6,\n"," 5,\n"," 3,\n"," 0,\n"," 5,\n"," 0,\n"," 5,\n"," 0,\n"," 8,\n"," 2,\n"," 6,\n"," 7,\n"," 3,\n"," 8,\n"," 2,\n"," 1,\n"," 7,\n"," 6,\n"," 7,\n"," 1,\n"," 0,\n"," 9,\n"," 5,\n"," 5,\n"," 0,\n"," 1,\n"," 7,\n"," 6,\n"," 9,\n"," 0,\n"," 4,\n"," 7,\n"," 7,\n"," 1,\n"," 5,\n"," 9,\n"," 4,\n"," 0,\n"," 8,\n"," 5,\n"," 9,\n"," 9,\n"," 6,\n"," 7,\n"," 1,\n"," 8,\n"," 3,\n"," 2,\n"," 3,\n"," 8,\n"," 2,\n"," 2,\n"," 4,\n"," 6,\n"," 0,\n"," 0,\n"," 5,\n"," 3,\n"," 8,\n"," 2,\n"," 3,\n"," 7,\n"," 2,\n"," 9,\n"," 3,\n"," 8,\n"," 7,\n"," 8,\n"," 2,\n"," 7,\n"," 9,\n"," 0,\n"," 2,\n"," 3,\n"," 2,\n"," 2,\n"," 2,\n"," 3,\n"," 3,\n"," 6,\n"," 2,\n"," 3,\n"," 2,\n"," 8,\n"," 0,\n"," 5,\n"," 5,\n"," 1,\n"," 4,\n"," 5,\n"," 6,\n"," 6,\n"," 2,\n"," 7,\n"," 0,\n"," 1,\n"," 7,\n"," 7,\n"," 8,\n"," 2,\n"," 9,\n"," 2,\n"," 2,\n"," 4,\n"," 2,\n"," 1,\n"," 1,\n"," 1,\n"," 6,\n"," 6,\n"," 6,\n"," 5,\n"," 1,\n"," 1,\n"," 7,\n"," 0,\n"," 4,\n"," 3,\n"," 3,\n"," 7,\n"," 1,\n"," 2,\n"," 3,\n"," 5,\n"," 5,\n"," 5,\n"," 6,\n"," 1,\n"," 4,\n"," 3,\n"," 7,\n"," 8,\n"," 8,\n"," 3,\n"," 6,\n"," 6,\n"," 2,\n"," 3,\n"," 0,\n"," 9,\n"," 4,\n"," 3,\n"," 8,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 5,\n"," 4,\n"," 9,\n"," 3,\n"," 1,\n"," 8,\n"," 9,\n"," 3,\n"," 9,\n"," 9,\n"," 2,\n"," 9,\n"," 4,\n"," 8,\n"," 2,\n"," 9,\n"," 8,\n"," 8,\n"," 1,\n"," 5,\n"," 3,\n"," 6,\n"," 8,\n"," 7,\n"," 6,\n"," 9,\n"," 8,\n"," 0,\n"," 6,\n"," 4,\n"," 0,\n"," 0,\n"," 2,\n"," 5,\n"," 8,\n"," 2,\n"," 0,\n"," 2,\n"," 7,\n"," 6,\n"," 9,\n"," 7,\n"," 1,\n"," 5,\n"," 5,\n"," 6,\n"," 6,\n"," 3,\n"," 6,\n"," 2,\n"," 4,\n"," 7,\n"," 0,\n"," 5,\n"," 6,\n"," 4,\n"," 6,\n"," 5,\n"," 2,\n"," 4,\n"," 6,\n"," 1,\n"," 6,\n"," 0,\n"," 4,\n"," 0,\n"," 3,\n"," 1,\n"," 8,\n"," 5,\n"," 4,\n"," 4,\n"," 1,\n"," 7,\n"," 3,\n"," 9,\n"," 4,\n"," 7,\n"," 9,\n"," 7,\n"," 3,\n"," 7,\n"," 2,\n"," 8,\n"," 4,\n"," 6,\n"," 6,\n"," 1,\n"," 2,\n"," 9,\n"," 0,\n"," 4,\n"," 8,\n"," 7,\n"," 3,\n"," 9,\n"," 8,\n"," 7,\n"," 7,\n"," 0,\n"," 2,\n"," 4,\n"," 1,\n"," 1,\n"," 4,\n"," 1,\n"," 5,\n"," 4,\n"," 0,\n"," 5,\n"," 6,\n"," 2,\n"," 8,\n"," 5,\n"," 0,\n"," 2,\n"," 1,\n"," 3,\n"," 5,\n"," 7,\n"," 3,\n"," 5,\n"," 1,\n"," 3,\n"," 5,\n"," ...]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_UNr72n-1a6t","executionInfo":{"status":"ok","timestamp":1626164784438,"user_tz":-60,"elapsed":20,"user":{"displayName":"Jacob Oliver","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_d3WZpZuOYfhw8lFJT1qT-x68z0UdHQ2wGVSH=s64","userId":"00067215047497340995"}},"outputId":"bef44573-470e-4567-8362-35d49cca50f8"},"source":["# number of classes\n","K = len(set(train_dataset.targets))\n","print(\"numer of classes: \", K)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["numer of classes:  10\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lVBqxnbM1cwt","executionInfo":{"status":"ok","timestamp":1626164784439,"user_tz":-60,"elapsed":8,"user":{"displayName":"Jacob Oliver","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_d3WZpZuOYfhw8lFJT1qT-x68z0UdHQ2wGVSH=s64","userId":"00067215047497340995"}}},"source":["# Data loader\n","# Useful because it automatically generates batches in the training loop\n","# and takes care of shuffling\n","\n","batch_size = 128\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WuIZPTau1fsN","executionInfo":{"status":"ok","timestamp":1626164786101,"user_tz":-60,"elapsed":1669,"user":{"displayName":"Jacob Oliver","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_d3WZpZuOYfhw8lFJT1qT-x68z0UdHQ2wGVSH=s64","userId":"00067215047497340995"}},"outputId":"163609c6-9d21-4b4e-fe1d-a35ec280643f"},"source":["# Make pne for testing\n","train_dataset_fixed = torchvision.datasets.CIFAR10(\n","    root='.',\n","    train=True,\n","    transform=transforms.ToTensor(),\n","    download=True\n",")\n","train_loader_fixed = torch.utils.data.DataLoader(\n","    dataset=train_dataset_fixed,\n","    batch_size=batch_size,\n","    shuffle=False\n",")"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ctWMhgNO1vIb","executionInfo":{"status":"ok","timestamp":1626164786102,"user_tz":-60,"elapsed":30,"user":{"displayName":"Jacob Oliver","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_d3WZpZuOYfhw8lFJT1qT-x68z0UdHQ2wGVSH=s64","userId":"00067215047497340995"}},"outputId":"31a132a2-12c8-4dd3-b1c9-657e606ce24e"},"source":["# the data transformer mapped the data to (0, 1)\n","# and also moved the colour channel before height/width\n","tmp_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                         batch_size=1,\n","                                         shuffle=True)\n","\n","for x, y in tmp_loader:\n","  print(x)\n","  print(x.shape)\n","  break"],"execution_count":9,"outputs":[{"output_type":"stream","text":["tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","          [0.0902, 0.0784, 0.0588,  ..., 0.6157, 0.0000, 0.0000],\n","          ...,\n","          [0.7059, 0.5882, 0.3922,  ..., 0.7804, 0.0000, 0.0000],\n","          [0.7333, 0.6510, 0.6471,  ..., 0.8157, 0.0000, 0.0000],\n","          [0.8392, 0.7804, 0.7765,  ..., 0.8039, 0.0000, 0.0000]],\n","\n","         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","          [0.0745, 0.0627, 0.0431,  ..., 0.5059, 0.0000, 0.0000],\n","          ...,\n","          [0.6667, 0.5529, 0.3608,  ..., 0.7333, 0.0000, 0.0000],\n","          [0.6941, 0.6196, 0.6235,  ..., 0.7882, 0.0000, 0.0000],\n","          [0.7961, 0.7451, 0.7490,  ..., 0.7843, 0.0000, 0.0000]],\n","\n","         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","          [0.0784, 0.0667, 0.0471,  ..., 0.3529, 0.0000, 0.0000],\n","          ...,\n","          [0.6118, 0.5294, 0.3647,  ..., 0.6863, 0.0000, 0.0000],\n","          [0.6510, 0.5843, 0.5961,  ..., 0.7529, 0.0000, 0.0000],\n","          [0.7490, 0.6902, 0.6902,  ..., 0.7569, 0.0000, 0.0000]]]])\n","torch.Size([1, 3, 32, 32])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DASqti5m1xv3","executionInfo":{"status":"ok","timestamp":1626164786104,"user_tz":-60,"elapsed":22,"user":{"displayName":"Jacob Oliver","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_d3WZpZuOYfhw8lFJT1qT-x68z0UdHQ2wGVSH=s64","userId":"00067215047497340995"}}},"source":["# Define the model\n","class CNN(nn.Module):\n","  def __init__(self, K):\n","    super(CNN, self).__init__()\n","\n","    # define the conv layers\n","    self.conv1 = nn.Sequential(\n","        nn.Conv2d(3, 32, kernel_size=3, padding=1),\n","        nn.ReLU(),\n","        nn.BatchNorm2d(32),\n","        nn.Conv2d(32, 32, kernel_size=3, padding=1),\n","        nn.ReLU(),\n","        nn.BatchNorm2d(32),\n","        nn.MaxPool2d(2),\n","    )\n","    self.conv2 = nn.Sequential(\n","        nn.Conv2d(32, 64, kernel_size=3, padding=1),\n","        nn.ReLU(),\n","        nn.BatchNorm2d(64),\n","        nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","        nn.ReLU(),\n","        nn.BatchNorm2d(64),\n","        nn.MaxPool2d(2),\n","    )\n","    self.conv3 = nn.Sequential(\n","        nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","        nn.ReLU(),\n","        nn.BatchNorm2d(128),\n","        nn.Conv2d(128, 128, kernel_size=3, padding=1),\n","        nn.ReLU(),\n","        nn.BatchNorm2d(128),\n","        nn.MaxPool2d(2),\n","    )\n","\n","    # Useful: https://pytorch.org/docs/stable/nn.html#torch.nn.MaxPool2d\n","    # H_out = H_in + 2p - 2 --> p = 1 if H_out = H_in\n","\n","    # Easy to calculate output\n","    # 32 > 16 > 8 > 4\n","\n","    # define the linear layers\n","    self.fc1 = nn.Linear(128 * 4 * 4, 1024)\n","    self.fc2 = nn.Linear(1024, K)\n","\n","  def forward(self, x):\n","    x = self.conv1(x)\n","    x = self.conv2(x)\n","    x = self.conv3(x)\n","    x = x.view(x.size(0), -1)\n","    x = F.dropout(x, p=0.5)\n","    x = F.relu(self.fc1(x))\n","    x = F.dropout(x, p=0.2)\n","    x = self.fc2(x)\n","    return x"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"3nv9FwBD3RgL","executionInfo":{"status":"ok","timestamp":1626164786105,"user_tz":-60,"elapsed":20,"user":{"displayName":"Jacob Oliver","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_d3WZpZuOYfhw8lFJT1qT-x68z0UdHQ2wGVSH=s64","userId":"00067215047497340995"}}},"source":["# Instantiate the model\n","model = CNN(K)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WI_tBHAG3UUC","executionInfo":{"status":"ok","timestamp":1626164795700,"user_tz":-60,"elapsed":9613,"user":{"displayName":"Jacob Oliver","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_d3WZpZuOYfhw8lFJT1qT-x68z0UdHQ2wGVSH=s64","userId":"00067215047497340995"}},"outputId":"cdf62dbb-3029-4e1c-cd09-c5846cdf125f"},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","model.to(device)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["cuda:0\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["CNN(\n","  (conv1): Sequential(\n","    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU()\n","    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (4): ReLU()\n","    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (conv2): Sequential(\n","    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU()\n","    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (4): ReLU()\n","    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (conv3): Sequential(\n","    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU()\n","    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (4): ReLU()\n","    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (fc1): Linear(in_features=2048, out_features=1024, bias=True)\n","  (fc2): Linear(in_features=1024, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"3RRj5LaG3fMo","executionInfo":{"status":"ok","timestamp":1626164795702,"user_tz":-60,"elapsed":17,"user":{"displayName":"Jacob Oliver","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_d3WZpZuOYfhw8lFJT1qT-x68z0UdHQ2wGVSH=s64","userId":"00067215047497340995"}}},"source":["# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters())"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"bQD-Mkjs3mXZ","executionInfo":{"status":"ok","timestamp":1626164795703,"user_tz":-60,"elapsed":15,"user":{"displayName":"Jacob Oliver","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_d3WZpZuOYfhw8lFJT1qT-x68z0UdHQ2wGVSH=s64","userId":"00067215047497340995"}}},"source":["# A function to encapsulate the training loop\n","def batch_gd(model, criterion, optimizer, train_loader, test_loader, epochs):\n","  train_losses = np.zeros(epochs)\n","  test_losses = np.zeros(epochs)\n","\n","  for it in range(epochs):\n","    model.train()\n","    t0 = datetime.now()\n","    train_loss = []\n","    for inputs, targets in train_loader:\n","      # move data to GPU\n","      inputs, targets = inputs.to(device), targets.to(device)\n","\n","      # zero the parameter gradients\n","      optimizer.zero_grad()\n","\n","      # Forward pass\n","      outputs = model(inputs)\n","      loss = criterion(outputs, targets)\n","\n","      # Backward and optimize\n","      loss.backward()\n","      optimizer.step()\n","\n","      train_loss.append(loss.item())\n","\n","    # Get train loss and test loss\n","    train_loss = np.mean(train_loss) # a little misleading\n","\n","    model.eval()\n","    test_loss = []\n","    for inputs, targets in test_loader:\n","      inputs, targets = inputs.to(device), targets.to(device)\n","      outputs = model(inputs)\n","      loss = criterion(outputs, targets)\n","      test_loss.append(loss.item())\n","    test_loss = np.mean(test_loss)\n","\n","    # Save losses\n","    train_losses[it] = train_loss\n","    test_losses[it] = test_loss\n","\n","    dt = datetime.now() - t0\n","    print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, \\\n","      Test Loss: {test_loss:.4f}, Duration: {dt}')\n","    \n","  return train_losses, test_losses"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ALM0ySq83rTR","outputId":"24dde58f-2ea8-4022-b954-203b9b20398f"},"source":["train_losses, test_losses = batch_gd(\n","    model, criterion, optimizer, train_loader, test_loader, epochs=80\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/80, Train Loss: 1.4382,       Test Loss: 1.2196, Duration: 0:00:41.582723\n","Epoch 2/80, Train Loss: 1.0389,       Test Loss: 0.9516, Duration: 0:00:42.238786\n","Epoch 3/80, Train Loss: 0.8684,       Test Loss: 0.7996, Duration: 0:00:41.247964\n","Epoch 4/80, Train Loss: 0.7661,       Test Loss: 0.7397, Duration: 0:00:41.341285\n","Epoch 5/80, Train Loss: 0.7011,       Test Loss: 0.6764, Duration: 0:00:41.287668\n","Epoch 6/80, Train Loss: 0.6519,       Test Loss: 0.6592, Duration: 0:00:41.400327\n","Epoch 7/80, Train Loss: 0.6201,       Test Loss: 0.8071, Duration: 0:00:41.741141\n","Epoch 8/80, Train Loss: 0.5819,       Test Loss: 0.5675, Duration: 0:00:41.624965\n","Epoch 9/80, Train Loss: 0.5574,       Test Loss: 0.5879, Duration: 0:00:41.575762\n","Epoch 10/80, Train Loss: 0.5353,       Test Loss: 0.5803, Duration: 0:00:41.477467\n","Epoch 11/80, Train Loss: 0.5213,       Test Loss: 0.5419, Duration: 0:00:41.433124\n","Epoch 12/80, Train Loss: 0.5017,       Test Loss: 0.5101, Duration: 0:00:41.435731\n","Epoch 13/80, Train Loss: 0.4809,       Test Loss: 0.5377, Duration: 0:00:41.420785\n","Epoch 14/80, Train Loss: 0.4701,       Test Loss: 0.5186, Duration: 0:00:41.455246\n","Epoch 15/80, Train Loss: 0.4565,       Test Loss: 0.5480, Duration: 0:00:41.525887\n","Epoch 16/80, Train Loss: 0.4419,       Test Loss: 0.5866, Duration: 0:00:41.427815\n","Epoch 17/80, Train Loss: 0.4342,       Test Loss: 0.5228, Duration: 0:00:41.339527\n","Epoch 18/80, Train Loss: 0.4192,       Test Loss: 0.4825, Duration: 0:00:41.290203\n","Epoch 19/80, Train Loss: 0.4102,       Test Loss: 0.4569, Duration: 0:00:41.211665\n","Epoch 20/80, Train Loss: 0.3970,       Test Loss: 0.4720, Duration: 0:00:41.238179\n","Epoch 21/80, Train Loss: 0.3922,       Test Loss: 0.4637, Duration: 0:00:41.374006\n","Epoch 22/80, Train Loss: 0.3862,       Test Loss: 0.4792, Duration: 0:00:41.525857\n","Epoch 23/80, Train Loss: 0.3719,       Test Loss: 0.4645, Duration: 0:00:41.291269\n","Epoch 24/80, Train Loss: 0.3671,       Test Loss: 0.4681, Duration: 0:00:41.470654\n","Epoch 25/80, Train Loss: 0.3618,       Test Loss: 0.4305, Duration: 0:00:41.328984\n","Epoch 26/80, Train Loss: 0.3531,       Test Loss: 0.4965, Duration: 0:00:41.079623\n","Epoch 27/80, Train Loss: 0.3460,       Test Loss: 0.4527, Duration: 0:00:41.584932\n","Epoch 28/80, Train Loss: 0.3405,       Test Loss: 0.4645, Duration: 0:00:41.785514\n","Epoch 29/80, Train Loss: 0.3316,       Test Loss: 0.4750, Duration: 0:00:41.776851\n","Epoch 30/80, Train Loss: 0.3313,       Test Loss: 0.4730, Duration: 0:00:41.550453\n","Epoch 31/80, Train Loss: 0.3234,       Test Loss: 0.4602, Duration: 0:00:41.588285\n","Epoch 32/80, Train Loss: 0.3215,       Test Loss: 0.4226, Duration: 0:00:41.473983\n","Epoch 33/80, Train Loss: 0.3129,       Test Loss: 0.4494, Duration: 0:00:41.368248\n","Epoch 34/80, Train Loss: 0.3068,       Test Loss: 0.4309, Duration: 0:00:41.455302\n","Epoch 35/80, Train Loss: 0.3003,       Test Loss: 0.4433, Duration: 0:00:41.467181\n","Epoch 36/80, Train Loss: 0.2994,       Test Loss: 0.4382, Duration: 0:00:41.480562\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eGD6cdRr3roK"},"source":["# Plot the train loss and test loss per iteration\n","plt.plot(train_losses, label='train loss')\n","plt.plot(test_losses, label='test loss')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N21f-oXq4QDB"},"source":["# Accuracy\n","\n","model.eval()\n","n_correct = 0.\n","n_total = 0.\n","for inputs, targets in train_loader:\n","  # move data to GPU\n","  inputs, targets = inputs.to(device), targets.to(device)\n","\n","  # Forward pass\n","  outputs = model(inputs)\n","\n","  # Get prediction\n","  # torch.max returns both max and argmax\n","  _, predictions = torch.max(outputs, 1)\n","\n","  # update counts\n","  n_correct += (predictions == targets).sum().item()\n","  n_total += targets.shape[0]\n","\n","train_acc = n_correct / n_total\n","\n","n_correct = 0.\n","n_total = 0.\n","for inputs, targets in test_loader:\n","  # move data to GPU\n","  inputs, targets = inputs.to(device), targets.to(device)\n","\n","  # Forward pass\n","  outputs = model(inputs)\n","\n","  # Get prediction\n","  # torch.max returns both max and argmax\n","  _, predictions = torch.max(outputs, 1)\n","\n","  # update counts\n","  n_correct += (predictions == targets).sum().item()\n","  n_total += targets.shape[0]\n","\n","test_acc = n_correct / n_total\n","print(f\"Train acc: {train_acc:.4f}, Test acc: {test_acc:.4f}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fOahWXeM4SYP"},"source":["# Plot confusion matrix\n","from sklearn.metrics import confusion_matrix\n","import numpy as np\n","import itertools\n","\n","def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","  \n","  \"\"\"\n","  This function prints and plots the confusion matrix.\n","  Normalization can be applied by setting 'normalize=True'.\n","  \"\"\"\n","  if normalize:\n","    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","    print(\"Normalized confusion matrix\")\n","  else:\n","    print(\"Confusion matrix, without normalization\")\n","\n","  print(cm)\n","\n","  plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","  plt.title(title)\n","  plt.colorbar()\n","  tick_marks = np.arange(len(classes))\n","  plt.xticks(tick_marks, classes, rotation=45)\n","  plt.yticks(tick_marks, classes)\n","\n","  fmt = '.2f' if normalize else 'd'\n","  thresh = cm.max() / 2.\n","  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","    plt.text(j, i, format(cm[i, j], fmt),\n","             horizontalalignment=\"center\",\n","             color=\"white\" if cm[i, j] > thresh else \"black\")\n","    \n","  plt.tight_layout()\n","  plt.ylabel(\"True label\")\n","  plt.xlabel(\"Predicted label\")\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gmZjViUm4Tex"},"source":["# Get all predictions in an array and plot confusion matrix\n","\n","x_test = test_dataset.data\n","y_test = np.array(test_dataset.targets)\n","p_test = np.array([])\n","for inputs, targets in test_loader:\n","  # Move data to GPU\n","  inputs, targets = inputs.to(device), targets.to(device)\n","\n","  # Forward pass\n","  outputs = model(inputs)\n","\n","  # Get prediction\n","  _, predictions = torch.max(outputs, 1)\n","\n","  # Update p_test\n","  p_test = np.concatenate((p_test, predictions.cpu().numpy()))\n","\n","cm = confusion_matrix(y_test, p_test)\n","plot_confusion_matrix(cm, list(range(10)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MWjT4htR4Utf"},"source":["# label mapping\n","labels = '''airplane\n","automobile\n","bird\n","cat\n","deer\n","dog\n","frog\n","horse\n","ship\n","truck'''.split()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1fJr2vrB4V81"},"source":["# Show some misclassified examples\n","p_test = p_test.astype(np.uint8)\n","misclassified_idx = np.where(p_test != y_test)[0]\n","i = np.random.choice(misclassified_idx)\n","plt.imshow(x_test[i].reshape(32,32,3))\n","plt.title(\"True label: %s Predicted: %s\" % (labels[y_test[i]], labels[p_test[i]]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"23iDFa8t4XT4"},"source":["from torchsummary import summary\n","summary(model, (3, 32, 32))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VcQUFjCe4tlg"},"source":[""],"execution_count":null,"outputs":[]}]}